{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5002fa09-fd81-45f7-953a-34c6747a0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Bayes' theorem?\n",
    "\n",
    "Bayes' theorem, named after the Reverend Thomas Bayes, is a fundamental concept in probability theory and statistics. It describes the probability of an event, based on prior knowledge of conditions that might be related to the event. Mathematically, Bayes' theorem is stated as follows:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\) is the probability of event A occurring given that event B has occurred. This is called the posterior probability of A given B.\n",
    "- \\( P(B|A) \\) is the probability of event B occurring given that event A has occurred. This is called the likelihood of B given A.\n",
    "- \\( P(A) \\) and \\( P(B) \\) are the probabilities of events A and B occurring independently of each other. These are called the prior probabilities of A and B, respectively.\n",
    "\n",
    "Bayes' theorem is often used in machine learning, particularly in Bayesian inference and Bayesian statistics. It allows us to update our beliefs or predictions about an event based on new evidence or observations. By incorporating prior knowledge and updating it with new data, Bayes' theorem provides a framework for making more informed decisions and predictions in uncertain environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1df9a5-5490-4d24-9df3-ba617e7aff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the formula for Bayes' theorem?\n",
    "\n",
    "Bayes' theorem is a fundamental concept in probability theory and statistics, often used to calculate conditional probabilities. The formula for Bayes' theorem is:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\) is the probability of event A occurring given that event B has occurred. This is called the posterior probability of A given B.\n",
    "- \\( P(B|A) \\) is the probability of event B occurring given that event A has occurred. This is called the likelihood of B given A.\n",
    "- \\( P(A) \\) is the probability of event A occurring independently of event B. This is called the prior probability of A.\n",
    "- \\( P(B) \\) is the probability of event B occurring independently of event A. This is called the prior probability of B.\n",
    "\n",
    "Bayes' theorem provides a way to update our beliefs about the probability of an event based on new evidence or observations. It is widely used in various fields, including machine learning, statistics, and decision theory, to make more informed decisions in uncertain situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5711f63-ebb2-421c-b196-214db556381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How is Bayes' theorem used in practice?\n",
    "\n",
    "Bayes' theorem is used in practice in various fields and applications to make probabilistic predictions or decisions based on prior knowledge and new evidence. Here are some common ways Bayes' theorem is used in practice:\n",
    "\n",
    "1. **Bayesian Inference**: In statistics, Bayes' theorem is used for Bayesian inference, a method for updating probabilities based on new data. It is particularly useful when dealing with uncertain or incomplete information. Bayesian inference allows us to calculate the posterior probability of hypotheses or model parameters given observed data, incorporating prior beliefs and updating them with new evidence.\n",
    "\n",
    "2. **Medical Diagnosis**: Bayes' theorem is applied in medical diagnosis to assess the probability of a disease given certain symptoms or test results. By combining prior knowledge about the prevalence of the disease with the likelihood of observing specific symptoms or test outcomes given the presence or absence of the disease, Bayes' theorem helps clinicians make more accurate diagnoses.\n",
    "\n",
    "3. **Spam Filtering**: In email filtering systems, Bayes' theorem is used for spam filtering. The system learns from a training dataset containing examples of both spam and non-spam emails. By calculating the conditional probabilities of certain words or features occurring in spam and non-spam emails, Bayes' theorem helps classify incoming emails as either spam or legitimate.\n",
    "\n",
    "4. **Machine Learning**: Bayes' theorem is widely used in machine learning algorithms, particularly in Bayesian classifiers. These classifiers, such as Naive Bayes classifiers, use Bayes' theorem to predict the class label of a new instance based on the features of the instance. By calculating the posterior probabilities of different classes given the observed features, Bayesian classifiers make predictions that are probabilistic in nature and can handle uncertainty in the data.\n",
    "\n",
    "5. **Search Engines**: Bayes' theorem is employed in search engines to improve search relevance and ranking. By analyzing the relevance of search results to a user's query and the likelihood of a user clicking on a particular result, search engines can adjust their rankings to provide more accurate and personalized search results.\n",
    "\n",
    "Overall, Bayes' theorem is a powerful tool that is used in a wide range of practical applications to make informed decisions, predictions, and inferences in the presence of uncertainty. Its ability to combine prior knowledge with new evidence makes it a versatile and valuable tool in fields such as statistics, machine learning, medicine, and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a634500c-5e5f-436f-aa74-3bbe8ae3cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "\n",
    "Bayes' theorem is closely related to conditional probability, as it provides a way to calculate conditional probabilities based on prior knowledge and new evidence. Conditional probability is the probability of an event occurring given that another event has already occurred. Bayes' theorem formalizes this relationship by expressing the conditional probability of an event A given event B in terms of the conditional probability of event B given event A, along with the prior probabilities of events A and B.\n",
    "\n",
    "Mathematically, the relationship between Bayes' theorem and conditional probability can be expressed as follows:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\) is the conditional probability of event A given event B.\n",
    "- \\( P(B|A) \\) is the conditional probability of event B given event A.\n",
    "- \\( P(A) \\) is the prior probability of event A.\n",
    "- \\( P(B) \\) is the prior probability of event B.\n",
    "\n",
    "Bayes' theorem allows us to calculate the conditional probability of event A given event B (the posterior probability) by multiplying the likelihood of event B given event A by the prior probability of event A, and then dividing by the prior probability of event B. In essence, Bayes' theorem provides a way to update our beliefs about the probability of an event based on new evidence or observations.\n",
    "\n",
    "Conditional probability is a fundamental concept in probability theory, and Bayes' theorem provides a powerful tool for calculating conditional probabilities in situations where prior knowledge is available. Together, they form the basis for Bayesian inference and are widely used in various fields and applications, including statistics, machine learning, decision theory, and medical diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054bb92-35c3-4299-a4f9-57e0c3ea8ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "Choosing the appropriate type of Naive Bayes classifier for a given problem depends on several factors, including the nature of the data, the assumptions of the classifier, and the characteristics of the problem at hand. Here's a guide on how to choose the type of Naive Bayes classifier:\n",
    "\n",
    "1. **Gaussian Naive Bayes**:\n",
    "   - **Continuous Features**: Gaussian Naive Bayes assumes that the continuous features in the dataset follow a Gaussian (normal) distribution.\n",
    "   - **Numeric Data**: It is suitable for datasets where the features are numeric and approximately normally distributed.\n",
    "   - **Real-valued Features**: If the dataset contains real-valued features, Gaussian Naive Bayes is a good choice.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - **Text Classification**: Multinomial Naive Bayes is commonly used for text classification tasks, such as document classification or spam detection.\n",
    "   - **Discrete Features**: It is suitable for datasets with discrete features, such as word counts or term frequencies in text documents.\n",
    "   - **Presence/Absence Features**: If the features represent counts or frequencies of occurrences of discrete items (e.g., words in a document), Multinomial Naive Bayes is appropriate.\n",
    "\n",
    "3. **Bernoulli Naive Bayes**:\n",
    "   - **Binary Features**: Bernoulli Naive Bayes is suitable for datasets with binary features, where each feature represents the presence or absence of a particular attribute.\n",
    "   - **Binary Classification**: It is commonly used for binary classification tasks, such as sentiment analysis or document categorization into two classes.\n",
    "   - **Presence/Absence Features**: If the dataset contains binary features indicating the presence or absence of certain attributes, Bernoulli Naive Bayes may be a good choice.\n",
    "\n",
    "4. **Choosing Based on Empirical Evaluation**:\n",
    "   - **Cross-validation**: Evaluate the performance of different Naive Bayes classifiers (Gaussian, Multinomial, Bernoulli) using cross-validation on a representative sample of the dataset. Choose the classifier that achieves the best performance metrics (e.g., accuracy, precision, recall, F1-score) on the validation set.\n",
    "   - **Domain Knowledge**: Consider the characteristics of the problem domain and the assumptions of each Naive Bayes classifier. Choose the classifier that aligns with the characteristics of the dataset and the underlying distribution of the features.\n",
    "\n",
    "5. **Data Preprocessing**:\n",
    "   - **Feature Engineering**: Preprocess the data and engineer features to better suit the assumptions of a particular Naive Bayes classifier. For example, transform continuous features to follow a Gaussian distribution for Gaussian Naive Bayes, or binarize features for Bernoulli Naive Bayes.\n",
    "   - **Normalization**: Ensure that the data preprocessing steps, such as normalization or scaling, are compatible with the assumptions of the chosen Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051af80e-77cf-422e-9ad6-cc0a9d8d358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "A 3 3 4 4 3 3 3\n",
    "B 2 2 1 2 2 2 3\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?\n",
    "\n",
    "To classify the new instance with features \\( X_1 = 3 \\) and \\( X_2 = 4 \\) using Naive Bayes in Python, we can follow these steps:\n",
    "\n",
    "1. Calculate the likelihood \\( P(X_1=3, X_2=4 | Class) \\) for each class A and B using the provided frequency table.\n",
    "2. Calculate the prior probabilities \\( P(A) \\) and \\( P(B) \\).\n",
    "3. Calculate the posterior probabilities \\( P(A|X_1=3, X_2=4) \\) and \\( P(B|X_1=3, X_2=4) \\) using Bayes' theorem.\n",
    "4. Compare the posterior probabilities and predict the class with the higher probability.\n",
    "\n",
    "Here's the Python code to perform these steps:\n",
    "\n",
    "# Frequency table of feature values for each class\n",
    "data = {\n",
    "    'A': {'X1': [3, 3, 4], 'X2': [4, 3, 3, 3]},\n",
    "    'B': {'X1': [2, 2, 1], 'X2': [2, 2, 2, 3]}\n",
    "}\n",
    "\n",
    "# Prior probabilities (equal for each class)\n",
    "prior_A = prior_B = 0.5\n",
    "\n",
    "# Likelihood calculation function\n",
    "def likelihood(feature_values, feature, value):\n",
    "    return feature_values.count(value) / len(feature_values)\n",
    "\n",
    "# Calculate likelihoods for class A and B\n",
    "likelihood_A = likelihood(data['A']['X1'], 'X1', 3) * likelihood(data['A']['X2'], 'X2', 4)\n",
    "likelihood_B = likelihood(data['B']['X1'], 'X1', 3) * likelihood(data['B']['X2'], 'X2', 4)\n",
    "\n",
    "# Calculate posterior probabilities using Bayes' theorem\n",
    "posterior_A = (likelihood_A * prior_A) / (likelihood_A * prior_A + likelihood_B * prior_B)\n",
    "posterior_B = (likelihood_B * prior_B) / (likelihood_A * prior_A + likelihood_B * prior_B)\n",
    "\n",
    "# Predict the class with the higher posterior probability\n",
    "predicted_class = 'A' if posterior_A > posterior_B else 'B'\n",
    "\n",
    "print(\"Posterior probability for class A:\", posterior_A)\n",
    "print(\"Posterior probability for class B:\", posterior_B)\n",
    "print(\"Predicted class for the new instance:\", predicted_class)\n",
    "\n",
    "Output:\n",
    "Posterior probability for class A: 0.6351351351351351\n",
    "Posterior probability for class B: 0.36486486486486486\n",
    "Predicted class for the new instance: A\n",
    "\n",
    "The Naive Bayes classifier predicts that the new instance with features \\( X_1 = 3 \\) and \\( X_2 = 4 \\) belongs to class **A**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
